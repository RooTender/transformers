<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

锔 Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Tokenizery

Tokenizer jest odpowiedzialny za przygotowanie danych wejciowych dla modelu. Biblioteka zawiera tokenizery dla wszystkich modeli. Wikszo tokenizer贸w jest dostpna w dw贸ch wersjach: penej implementacji w Pythonie i "Fast" (pl. *szybkiej*) implementacji opartej na bibliotece Rust [ Tokenizers](https://github.com/huggingface/tokenizers). "Fast" implementacje pozwalaj na:

1. znaczne przyspieszenie, w szczeg贸lnoci podczas tokenizacji wsadowej,
2. dodatkowe metody mapowania midzy oryginalnym cigiem znak贸w (znakami i sowami) a przestrzeni token贸w (np. uzyskanie indeksu tokena zawierajcego dany znak lub zakres znak贸w odpowiadajcych danemu tokenowi).

Klasy bazowe [`PreTrainedTokenizer`] i [`PreTrainedTokenizerFast`] implementuj wsp贸lne metody kodowania cig贸w znak贸w w danych wejciowych modelu (patrz poni偶ej) oraz instancjonowania/zapisywania tokenizer贸w pythonowskich i "Fast" na bazie danych z lokalnego pliku, katalogu albo z wstpnie wytrenowanego tokenizera dostarczonego przez bibliotek (pobranej z repozytorium AWS S3 firmy HuggingFace). Oba opieraj si na [`~tokenization_utils_base.PreTrainedTokenizerBase`], kt贸ry zawiera zwyke metody, oraz [`~tokenization_utils_base.SpecialTokensMixin`].

Zatem [`PreTrainedTokenizer`] i [`PreTrainedTokenizerFast`] implementuj g贸wne metody korzystania ze wszystkich tokenizer贸w:

- Tokenizacja (dzielenie cig贸w znak贸w na cigi token贸w pod-s贸w), konwersja cig贸w token贸w na identyfikatory i z powrotem oraz kodowanie/dekodowanie (tj. tokenizacja i konwersja na liczby cakowite).
- Dodawanie nowych token贸w do sownika w spos贸b niezale偶ny od struktury bazowej (BPE, SentencePiece...).
- Zarzdzanie specjalnymi tokenami (takimi jak maska, pocztek zdania itp.): dodawanie ich, przypisywanie ich do atrybut贸w w tokenizatorze w celu atwego dostpu i upewnianie si, 偶e nie zostan podzielone podczas tokenizacji.

[`BatchEncoding`] przechowuje dane wyjciowe metod kodowania [`~tokenization_utils_base.PreTrainedTokenizerBase`] (`__call__`, `encode_plus` i `batch_encode_plus`) i pochodzi ze sownika Pythona. Kiedy tokenizator jest tokenizatorem czysto pythonowym, klasa ta zachowuje si jak standardowy sownik pythonowy i przechowuje r贸偶ne dane wejciowe modelu obliczone przez te metody (`input_ids`, `attention_mask`...). Gdy tokenizer jest "Fast" tokenizerem (tj. wspieranym przez HuggingFace [biblioteka tokenizer贸w](https://github.com/huggingface/tokenizers)), klasa ta zapewnia dodatkowo kilka zaawansowanych metod dopasowania, kt贸re mo偶na wykorzysta do mapowania midzy oryginalnym cigiem znak贸w (znakami i sowami) a przestrzeni token贸w (np. uzyskiwanie indeksu tokenu zawierajcego dany znak lub zakresu znak贸w odpowiadajcych danemu tokenowi).


## PreTrainedTokenizer

[[autodoc]] PreTrainedTokenizer
    - __call__
    - add_tokens
    - add_special_tokens
    - apply_chat_template
    - batch_decode
    - decode
    - encode
    - push_to_hub
    - all

## PreTrainedTokenizerFast

[`PreTrainedTokenizerFast`] zale偶 od biblioteki [tokenizers](https://huggingface.co/docs/tokenizers). Tokenizery uzyskane z biblioteki  Tokenizers, kt贸r mo偶na bardzo atwo wczyta do  Transformers. Zapoznaj si ze stron [U偶ywanie tokenizer贸w z  Tokenizers](../fast_tokenizers) aby zrozumie, jak to si robi.

[[autodoc]] PreTrainedTokenizerFast
    - __call__
    - add_tokens
    - add_special_tokens
    - apply_chat_template
    - batch_decode
    - decode
    - encode
    - push_to_hub
    - all

## BatchEncoding

[[autodoc]] BatchEncoding
